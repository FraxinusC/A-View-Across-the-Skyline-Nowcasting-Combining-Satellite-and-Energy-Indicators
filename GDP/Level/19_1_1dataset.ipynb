{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e468483e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fraxi\\.conda\\envs\\DL\\Lib\\site-packages\\torch\\__init__.py:274\u001b[0m\n\u001b[0;32m    270\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    272\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 274\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fraxi\\.conda\\envs\\DL\\Lib\\site-packages\\torch\\__init__.py:250\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    248\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 250\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "import rasterio\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from rasterio.enums import Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "def preprocess_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Applying the specified transformations\n",
    "    df['Time'] = df['Year'].apply(lambda x: datetime.datetime(x, 1, 1))\n",
    "\n",
    "    # Log transformation \n",
    "    cols_to_log_transform = ['GDP', 'Population', 'Personalincome', 'Percapitapersonalincome', 'econs', 'aland', 'awater','shape_area','shape_leng','Natural Gas Delivered to Consumers in California (Including Vehicle Fuel)  Million Cubic Feet']\n",
    "    df[cols_to_log_transform] = df[cols_to_log_transform].apply(np.log)\n",
    "\n",
    "    df.sort_values(by=['GeoFIPS', 'Time'], inplace=True)\n",
    "    # delte the 21 missing values\n",
    "    df = df.groupby('GeoFIPS').filter(lambda x: len(x) == 20)\n",
    "    \n",
    "    df_cleaned = df.copy()\n",
    "\n",
    "    # Displaying the first few rows of the processed data\n",
    "    df_cleaned.head()\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EconomicDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, economic_features, img_size, img_augmented_size, scale_factor=0.6,transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.economic_features = economic_features\n",
    "        self.img_size = img_size\n",
    "        self.img_augmented_size = img_augmented_size\n",
    "        self.transform = transform\n",
    "        self.years = dataframe['Year']  \n",
    "        self.geo_fips = dataframe['GeoFIPS']\n",
    "        self.scale_factor = scale_factor  \n",
    "\n",
    "    def read_and_resize_image(self, image_path):\n",
    "        with rasterio.open(image_path) as src:\n",
    "            # read the image and resize it\n",
    "            data = src.read(\n",
    "                out_shape=(\n",
    "                    src.count,\n",
    "                    int(src.height * self.scale_factor),\n",
    "                    int(src.width * self.scale_factor)\n",
    "                ),\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "            data = data.astype('float32')  \n",
    "            nan_mask = np.isnan(data)\n",
    "            data[nan_mask] = 0  \n",
    "            return data\n",
    "    def random_transform(self, image):\n",
    "\n",
    "        image = torch.from_numpy(image).float()\n",
    "        image = F.interpolate(image.unsqueeze(0), size=(self.img_size, self.img_size), mode='bilinear', align_corners=False).squeeze(0)\n",
    "        image = torch.clamp(image, 0, 1)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.dataframe.iloc[idx, 0])\n",
    "        with rasterio.open(img_path) as src:\n",
    "            image = src.read()  \n",
    "            image = image.astype('float32') \n",
    "            nan_mask = np.isnan(image)\n",
    "            image[nan_mask] = 0 \n",
    "\n",
    "        image = self.random_transform(image)\n",
    "\n",
    "        economic_features = self.economic_features[idx].float()  \n",
    "        label = self.dataframe.iloc[idx]['GDP'] \n",
    "        year = self.years.iloc[idx]\n",
    "        geo_fips = self.geo_fips.iloc[idx]\n",
    "        return image, economic_features, label, year, geo_fips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset creation\n",
    "def create_datasets(df_cleaned, img_size, img_augmented_size):\n",
    "    # group by 'GeoFIPS' and split into train, validation, and test sets\n",
    "    grouped = df_cleaned.groupby('GeoFIPS')\n",
    "    train_df_list = []\n",
    "    val_df_list = []\n",
    "    test_df_list = []\n",
    "\n",
    "    for _, group in grouped:\n",
    "        group = group.sort_values(by='Year')\n",
    "        total_years = len(group)\n",
    "\n",
    "        train_split = int(0.9 * total_years)\n",
    "        val_split = train_split + int(0.05 * total_years)\n",
    "\n",
    "        train_df_list.append(group.iloc[:train_split])\n",
    "        val_df_list.append(group.iloc[train_split:val_split])\n",
    "        test_df_list.append(group.iloc[val_split:])\n",
    "\n",
    "    train_df = pd.concat(train_df_list)\n",
    "    val_df = pd.concat(val_df_list)\n",
    "    test_df = pd.concat(test_df_list)\n",
    "    scaler = MinMaxScaler()\n",
    "    columns_to_scale = ['econs','Population', 'Personalincome', 'Percapitapersonalincome', 'aland', 'awater','shape_area','shape_leng','Natural Gas Delivered to Consumers in California (Including Vehicle Fuel)  Million Cubic Feet','intptlat','intptlon']\n",
    "    scaler.fit(train_df[columns_to_scale])\n",
    "\n",
    "    train_features = scaler.transform(train_df[columns_to_scale])\n",
    "    test_features = scaler.transform(test_df[columns_to_scale])\n",
    "    val_features = scaler.transform(val_df[columns_to_scale])\n",
    "\n",
    "    train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n",
    "    test_features_tensor = torch.tensor(test_features, dtype=torch.float32)\n",
    "    val_features_tensor = torch.tensor(val_features, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = EconomicDataset(train_df, \"<train_image_path>\", train_features_tensor, img_size, img_augmented_size)\n",
    "    val_dataset = EconomicDataset(val_df, \"<validation_image_path>\", val_features_tensor, img_size, img_augmented_size)\n",
    "    test_dataset = EconomicDataset(test_df, \"<test_image_path>\", test_features_tensor, img_size, img_augmented_size)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_dataset(dataset, save_path):\n",
    "    processed_data = []\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        image, econ_features, label, year, geo_fips = dataset[i]\n",
    "        if torch.isnan(econ_features).any():\n",
    "            econ_features[torch.isnan(econ_features)] = 0\n",
    "\n",
    "        processed_data.append((image, econ_features, label, year, geo_fips))\n",
    "\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "\n",
    "    print(f\"Saved processed data to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513422c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = preprocess_data(\"E://Nowcasting Code//Data//Label//2001_2020.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ebfff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 658/658 [19:18<00:00,  1.76s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to GDP01_20_train_dataset.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [04:11<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to GDP01_20_val_dataset.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [04:13<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to GDP01_20_test_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(df_cleaned, 512, 512)\n",
    "\n",
    "process_and_save_dataset(train_dataset, \"GDP01_18_train_dataset.pkl\")\n",
    "process_and_save_dataset(val_dataset, \"GDP18_19_val_dataset.pkl\")\n",
    "process_and_save_dataset(test_dataset, \"GDP19_20_test_dataset.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
